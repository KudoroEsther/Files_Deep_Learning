{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84ced37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21376593",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"C:\\Users\\owner\\Desktop\\Files_Deep_Learning\\NeuraGuide\\AI_Tools.csv\"\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "df = load_data(url)\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6ce8ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tool Name', 'Category', 'Primary Function', 'Description', 'Website',\n",
       "       'Pricing Model', 'Key Features', 'Target Users', 'Launch Year',\n",
       "       'Company', 'category_rank', 'ID', 'Category_code', 'average_rating',\n",
       "       'review_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f032e5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Primary Function</th>\n",
       "      <th>Description</th>\n",
       "      <th>Website</th>\n",
       "      <th>Pricing Model</th>\n",
       "      <th>Key Features</th>\n",
       "      <th>Target Users</th>\n",
       "      <th>Launch Year</th>\n",
       "      <th>Company</th>\n",
       "      <th>category_rank</th>\n",
       "      <th>ID</th>\n",
       "      <th>Category_code</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaedim</td>\n",
       "      <td>3D</td>\n",
       "      <td>3D</td>\n",
       "      <td>Transform 2D images into high-quality 3D model...</td>\n",
       "      <td>https://www.kaedim3d.com</td>\n",
       "      <td>Paid</td>\n",
       "      <td>See website</td>\n",
       "      <td>General</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tool Name Category Primary Function  \\\n",
       "0    Kaedim       3D               3D   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Transform 2D images into high-quality 3D model...   \n",
       "\n",
       "                    Website Pricing Model Key Features Target Users  \\\n",
       "0  https://www.kaedim3d.com          Paid  See website      General   \n",
       "\n",
       "  Launch Year  Company  category_rank  ID  Category_code  average_rating  \\\n",
       "0     Unknown  Unknown              1   1              0             0.0   \n",
       "\n",
       "   review_count  \n",
       "0             0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aefed848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54910, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53239363",
   "metadata": {},
   "source": [
    "## Identify and Remove Duplicate Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d7b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicates\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(df):\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Duplicated rows: {duplicates}\")\n",
    "        # df = df.drop_duplicates(inplace=True)\n",
    "        # dropped = df.duplicated().sum()\n",
    "        # print(f\"\\nDuplicates: {dropped}\")\n",
    "    else:\n",
    "        print(\"There are no duplicates\")\n",
    "\n",
    "check_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf527bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicates\n"
     ]
    }
   ],
   "source": [
    "# Detect duplicate entries using Tool Name, Company, and Website. Decide which record to keep and remove the rest using deterministic logic.\n",
    "\n",
    "class DuplicateHandler:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.duplicate_keys = ['Tool Name', 'Company', 'Website']\n",
    "        \n",
    "    def find_duplicates(self):\n",
    "        \"\"\"Identify duplicate groups\"\"\"\n",
    "        return self.df[self.df.duplicated(subset=self.duplicate_keys, keep=False)]\n",
    "    \n",
    "    def rank_records(self, group):\n",
    "        \"\"\"Score records: higher is better\"\"\"\n",
    "        scores = pd.DataFrame(index=group.index)\n",
    "        scores['completeness'] = group.notna().sum(axis=1)\n",
    "        scores['reviews'] = group['review_count'].fillna(0)\n",
    "        scores['rating'] = group['average_rating'].fillna(0)\n",
    "        scores['recency'] = group['Launch Year'].fillna(0)\n",
    "        scores['position'] = -np.arange(len(group))  # negative for ascending\n",
    "        return scores.sum(axis=1).idxmax()\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Keep best record per duplicate group\"\"\"\n",
    "        dupes = self.find_duplicates()\n",
    "        if dupes.empty:\n",
    "            print(\"There are no duplicates\")\n",
    "            return self.df, pd.DataFrame()\n",
    "        \n",
    "        keep_indices = dupes.groupby(self.duplicate_keys, dropna=False).apply(self.rank_records)\n",
    "        removed = self.df[self.df.index.isin(dupes.index) & ~self.df.index.isin(keep_indices)]\n",
    "        cleaned = self.df[~self.df.index.isin(dupes.index) | self.df.index.isin(keep_indices)]\n",
    "        \n",
    "        return cleaned.reset_index(drop=True), removed\n",
    "\n",
    "# Usage:\n",
    "handler = DuplicateHandler(df)\n",
    "cleaned_df, removed_records = handler.remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b667c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54910, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dac611",
   "metadata": {},
   "source": [
    "## Identify rows missing essential information such as Tool Name, Category, or Website. Flag or remove records based on defined rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             missing_count  missing_pct\n",
      "Launch Year           1420         2.59\n"
     ]
    }
   ],
   "source": [
    "class MissingDataHandler:\n",
    "    def __init__(self, df, essential_fields=None, flag_fields=None):\n",
    "        self.df = df.copy()\n",
    "        self.essential_fields = essential_fields or ['Tool Name', 'Category', 'Website']\n",
    "        self.flag_fields = flag_fields or ['Description', 'Primary Function', 'Company']\n",
    "        \n",
    "    def find_missing_essential(self):\n",
    "        \"\"\"Find rows with missing essential fields\"\"\"\n",
    "        mask = self.df[self.essential_fields].isna().any(axis=1)\n",
    "        return self.df[mask]\n",
    "    \n",
    "    def find_missing_flagged(self):\n",
    "        \"\"\"Find rows with missing flagged fields (warning only)\"\"\"\n",
    "        mask = self.df[self.flag_fields].isna().any(axis=1)\n",
    "        return self.df[mask]\n",
    "    \n",
    "    def get_missing_summary(self):\n",
    "        \"\"\"Summary of missing data by column\"\"\"\n",
    "        summary = pd.DataFrame({\n",
    "            'missing_count': self.df.isna().sum(),\n",
    "            'missing_pct': (self.df.isna().sum() / len(self.df) * 100).round(2)\n",
    "        })\n",
    "        return summary[summary['missing_count'] > 0].sort_values('missing_count', ascending=False)\n",
    "    \n",
    "    def remove_incomplete(self):\n",
    "        \"\"\"Remove rows missing essential fields\"\"\"\n",
    "        incomplete = self.find_missing_essential()\n",
    "        cleaned = self.df[~self.df.index.isin(incomplete.index)]\n",
    "        return cleaned.reset_index(drop=True), incomplete\n",
    "    \n",
    "    def flag_incomplete(self):\n",
    "        \"\"\"Add flag column for incomplete records\"\"\"\n",
    "        df_flagged = self.df.copy()\n",
    "        df_flagged['missing_essential'] = self.df[self.essential_fields].isna().any(axis=1)\n",
    "        df_flagged['missing_flagged'] = self.df[self.flag_fields].isna().any(axis=1)\n",
    "        return df_flagged\n",
    "\n",
    "# Usage:\n",
    "missing_handler = MissingDataHandler(df)\n",
    "print(missing_handler.get_missing_summary())\n",
    "cleaned_df, removed_records = missing_handler.remove_incomplete()\n",
    "# or\n",
    "flagged_df = missing_handler.flag_incomplete()\n",
    "# print(flagged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd695cc",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class URLValidator:\n",
    "    def __init__(self, df, url_column='Website'):\n",
    "        self.df = df.copy()\n",
    "        self.url_column = url_column\n",
    "        \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is properly formatted\"\"\"\n",
    "        if pd.isna(url) or not isinstance(url, str):\n",
    "            return False\n",
    "        \n",
    "        # Basic pattern check\n",
    "        url = url.strip()\n",
    "        if not re.match(r'^https?://', url, re.IGNORECASE):\n",
    "            url = 'http://' + url\n",
    "        \n",
    "        try:\n",
    "            import validators\n",
    "            return validators.url(url) is True\n",
    "        except ImportError:\n",
    "            # Fallback to regex if validators not installed\n",
    "            pattern = re.compile(\n",
    "                r'^https?://'\n",
    "                r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+[A-Z]{2,6}\\.?|'\n",
    "                r'localhost|'\n",
    "                r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'\n",
    "                r'(?::\\d+)?'\n",
    "                r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "            return bool(pattern.match(url))\n",
    "    \n",
    "    def validate_urls(self):\n",
    "        \"\"\"Validate all URLs and return results\"\"\"\n",
    "        results = self.df.copy()\n",
    "        results['url_valid'] = results[self.url_column].apply(self.is_valid_url)\n",
    "        results['url_missing'] = results[self.url_column].isna()\n",
    "        return results\n",
    "    \n",
    "    def get_invalid_urls(self):\n",
    "        \"\"\"Get records with invalid URLs\"\"\"\n",
    "        validated = self.validate_urls()\n",
    "        return validated[~validated['url_valid'] & ~validated['url_missing']]\n",
    "    \n",
    "    def check_reachability(self, timeout=5, max_workers=10):\n",
    "        \"\"\"Check if URLs are reachable (optional)\"\"\"\n",
    "        import requests\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        \n",
    "        def check_url(url):\n",
    "            if pd.isna(url):\n",
    "                return None\n",
    "            try:\n",
    "                url = url.strip()\n",
    "                if not re.match(r'^https?://', url, re.IGNORECASE):\n",
    "                    url = 'http://' + url\n",
    "                response = requests.head(url, timeout=timeout, allow_redirects=True)\n",
    "                return response.status_code < 400\n",
    "            except:\n",
    "                return False\n",
    "        \n",
    "        urls = self.df[self.url_column].dropna().unique()\n",
    "        reachability = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(check_url, url): url for url in urls}\n",
    "            for future in as_completed(futures):\n",
    "                url = futures[future]\n",
    "                reachability[url] = future.result()\n",
    "        \n",
    "        results = self.df.copy()\n",
    "        results['url_reachable'] = results[self.url_column].map(reachability)\n",
    "        return results\n",
    "    \n",
    "    def clean_urls(self):\n",
    "        \"\"\"Remove records with invalid URLs\"\"\"\n",
    "        validated = self.validate_urls()\n",
    "        cleaned = validated[validated['url_valid'] | validated['url_missing']]\n",
    "        invalid = validated[~validated['url_valid'] & ~validated['url_missing']]\n",
    "        return cleaned.drop(columns=['url_valid', 'url_missing']).reset_index(drop=True), invalid\n",
    "\n",
    "# Usage:\n",
    "# validator = URLValidator(df)\n",
    "# print(f\"Invalid URLs: {len(validator.get_invalid_urls())}\")\n",
    "# cleaned_df, invalid_urls = validator.clean_urls()\n",
    "# \n",
    "# # Optional: Check reachability (slower)\n",
    "# reachable_df = validator.check_reachability()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
