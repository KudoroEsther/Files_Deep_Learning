{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\Desktop\\Files_Deep_Learning\\a_deep\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_classic.chains import create_retrieval_chains\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e203e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"paid_api\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LinkedInRAGAssistant:\n",
    "    \"\"\"RAG Assistant for answering questions about a LinkedIn profile\"\"\"\n",
    "    \n",
    "    def __init__(self, persist_directory: str = \"./chroma_db\", \n",
    "                 documents_directory: str = \"./documents\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG Assistant\n",
    "        \n",
    "        Args:\n",
    "            persist_directory: Directory to store ChromaDB\n",
    "            documents_directory: Directory containing LinkedIn profile documents\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory\n",
    "        self.documents_directory = documents_directory\n",
    "        self.chat_history: List[Tuple[str, str]] = []\n",
    "        \n",
    "        # Initialize embeddings (using free HuggingFace embeddings)\n",
    "        self.embeddings = OpenAIEmbeddings(openai_api_key = api_key)\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "        self.chain = None\n",
    "        \n",
    "    def load_and_process_documents(self, chunk_size: int = 1000, \n",
    "                                   chunk_overlap: int = 200) -> List:\n",
    "        \"\"\"\n",
    "        Load and process documents from the documents directory\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Size of each text chunk\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of processed document chunks\n",
    "        \"\"\"\n",
    "        print(f\"Loading documents from {self.documents_directory}...\")\n",
    "        \n",
    "        # Load all text files from directory\n",
    "        loader = DirectoryLoader(\n",
    "            self.documents_directory,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_vector_store(self, chunks: List) -> None:\n",
    "        \"\"\"\n",
    "        Create and persist vector store with document embeddings\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of document chunks to embed\n",
    "        \"\"\"\n",
    "        print(\"Creating vector store with embeddings...\")\n",
    "        \n",
    "        # Create ChromaDB vector store\n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.persist_directory\n",
    "        )\n",
    "        \n",
    "        print(f\"Vector store created and persisted to {self.persist_directory}\")\n",
    "        \n",
    "    def load_existing_vector_store(self) -> bool:\n",
    "        \"\"\"\n",
    "        Load existing vector store from disk\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vector_store = Chroma(\n",
    "                persist_directory=self.persist_directory,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            print(\"Loaded existing vector store\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load existing vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_retriever(self, k: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        Setup retriever from vector store\n",
    "        \n",
    "        Args:\n",
    "            k: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Vector store not initialized. Call create_vector_store first.\")\n",
    "        \n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": k}\n",
    "        )\n",
    "        print(f\"Retriever configured to fetch top {k} documents\")\n",
    "    \n",
    "    def create_rag_chain(self, api_key: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Create RAG chain using LCEL with conversational memory\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API key (if None, uses environment variable)\n",
    "        \"\"\"\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Retriever not initialized. Call setup_retriever first.\")\n",
    "        \n",
    "        # Initialize LLM\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        \n",
    "        # Create custom prompt with memory\n",
    "        system_prompt = \"\"\"You are an AI assistant helping to answer questions about Esther Kudoro's LinkedIn profile and professional background.\n",
    "\n",
    "Use the following pieces of context from the profile to answer the question. If you cannot find the answer in the context, say so - don't make up information.\n",
    "\n",
    "Always cite your sources by mentioning which part of the profile the information comes from (e.g., \"According to the experience section...\" or \"Based on the skills listed...\").\n",
    "\n",
    "Context from LinkedIn profile:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Answer the question conversationally, maintaining context from previous exchanges.\"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # Create the RAG chain using LCEL\n",
    "        def format_docs(docs):\n",
    "            \"\"\"Format retrieved documents with source information\"\"\"\n",
    "            formatted = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                source = doc.metadata.get('source', 'Unknown')\n",
    "                formatted.append(f\"[Source {i}: {source}]\\n{doc.page_content}\")\n",
    "            return \"\\n\\n\".join(formatted)\n",
    "        \n",
    "        def format_chat_history(history):\n",
    "            \"\"\"Format chat history for the prompt\"\"\"\n",
    "            if not history:\n",
    "                return \"No previous conversation\"\n",
    "            \n",
    "            formatted = []\n",
    "            for human, ai in history[-3:]:  # Keep last 3 exchanges\n",
    "                formatted.append(f\"Human: {human}\\nAssistant: {ai}\")\n",
    "            return \"\\n\\n\".join(formatted)\n",
    "        \n",
    "        # Build the chain with LCEL\n",
    "        self.chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | format_docs,\n",
    "                \"chat_history\": lambda x: format_chat_history(self.chat_history),\n",
    "                \"input\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        print(\"RAG chain created with conversational memory\")\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Ask a question to the RAG assistant\n",
    "        \n",
    "        Args:\n",
    "            question: The question to ask\n",
    "            \n",
    "        Returns:\n",
    "            The assistant's answer with sources\n",
    "        \"\"\"\n",
    "        if self.chain is None:\n",
    "            raise ValueError(\"RAG chain not initialized. Call create_rag_chain first.\")\n",
    "        \n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        \n",
    "        # Get answer from chain\n",
    "        answer = self.chain.invoke(question)\n",
    "        \n",
    "        # Update chat history\n",
    "        self.chat_history.append((question, answer))\n",
    "        \n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def clear_history(self) -> None:\n",
    "        \"\"\"Clear chat history\"\"\"\n",
    "        self.chat_history = []\n",
    "        print(\"Chat history cleared\")\n",
    "    \n",
    "    def initialize_from_scratch(self, chunk_size: int = 1000, \n",
    "                               chunk_overlap: int = 200) -> None:\n",
    "        \"\"\"\n",
    "        Complete initialization pipeline from scratch\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        # Load and process documents\n",
    "        chunks = self.load_and_process_documents(chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Create vector store\n",
    "        self.create_vector_store(chunks)\n",
    "        \n",
    "        # Setup retriever\n",
    "        self.setup_retriever()\n",
    "        \n",
    "        # Create RAG chain\n",
    "        self.create_rag_chain()\n",
    "        \n",
    "        print(\"\\nâœ… RAG Assistant fully initialized!\")\n",
    "    \n",
    "    def initialize_from_existing(self) -> None:\n",
    "        \"\"\"Initialize from existing vector store\"\"\"\n",
    "        # Load existing vector store\n",
    "        if not self.load_existing_vector_store():\n",
    "            raise ValueError(\"No existing vector store found. Use initialize_from_scratch instead.\")\n",
    "        \n",
    "        # Setup retriever\n",
    "        self.setup_retriever()\n",
    "        \n",
    "        # Create RAG chain\n",
    "        self.create_rag_chain()\n",
    "        \n",
    "        print(\"\\nRAG Assistant initialized from existing data!\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function demonstrating the RAG assistant usage\"\"\"\n",
    "    \n",
    "    print(\"LinkedIn Profile RAG Assistant\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize assistant\n",
    "    assistant = LinkedInRAGAssistant()\n",
    "    \n",
    "    # Check if vector store already exists\n",
    "    if os.path.exists(assistant.persist_directory):\n",
    "        print(\"\\n Found existing vector store. Loading...\")\n",
    "        try:\n",
    "            assistant.initialize_from_existing()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing store: {e}\")\n",
    "            print(\"Creating new vector store...\")\n",
    "            assistant.initialize_from_scratch()\n",
    "    else:\n",
    "        print(\"\\nNo existing vector store found. Creating new one...\")\n",
    "        print(\"Make sure you have populated the linkedin_docs directory with content!\")\n",
    "        assistant.initialize_from_scratch()\n",
    "    \n",
    "    # Example questions to test the system\n",
    "    test_questions = [\n",
    "        \"What is Esther Kudoro's professional background?\",\n",
    "        \"What are her key skills?\",\n",
    "        \"What experience does she have?\",\n",
    "        \"Can you tell me about her education?\",\n",
    "        \"What projects has she worked on?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting with Example Questions\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for question in test_questions[:2]:  # Test with first 2 questions\n",
    "        answer = assistant.ask(question)\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nRAG Assistant ready for interactive use!\")\n",
    "    print(\"Use assistant.ask('your question') to query the system\")\n",
    "    \n",
    "    return assistant\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main function\n",
    "    assistant = main()\n",
    "    \n",
    "    # Interactive mode example\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting interactive session...\")\n",
    "    print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
