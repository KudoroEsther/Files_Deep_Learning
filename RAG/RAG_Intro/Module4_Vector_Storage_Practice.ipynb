{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dec7052",
   "metadata": {},
   "source": [
    "## FAISS Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a76b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\Desktop\\Files_Deep_Learning\\a_deep\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"FAISS version: {faiss.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227b8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\",\n",
    "    \"Data visualization helps communicate insights from complex datasets.\",\n",
    "    \"Cloud computing provides on-demand access to computing resources.\",\n",
    "    \"Cybersecurity protects systems and networks from digital attacks.\",\n",
    "    \"Blockchain technology enables secure, decentralized transactions.\",\n",
    "    \"Quantum computing uses quantum mechanics to solve complex problems.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8744e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be8ee5",
   "metadata": {},
   "source": [
    "### FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45e6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors in index: 10\n"
     ]
    }
   ],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "#Create FAISS index (IndexFlatL2 = exact search with L2 distance)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "index.add(embeddings)\n",
    "print(f\"Total vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e61b0",
   "metadata": {},
   "source": [
    "### Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8c9ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is artificial intelligence and machine learning?\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. (Distance: 0.9079)\n",
      "    Deep learning is a subset of machine learning using multi-layered neural networks.\n",
      "\n",
      "2. (Distance: 1.2202)\n",
      "    Machine learning models require large amounts of training data to perform well.\n",
      "\n",
      "3. (Distance: 1.2355)\n",
      "    Natural language processing enables computers to understand human language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is artificial intelligence and machine learning?\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "k=3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results:\\n\")\n",
    "\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0]),1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"    {documents[idx]}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862da3a",
   "metadata": {},
   "source": [
    "### Using Cosine Similarity with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec39a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is artificial intelligence and machine learning?\n",
      "\n",
      "Top 3 results with cosine similarity:\n",
      "\n",
      "1. (Similarity: 0.5460)\n",
      "   Deep learning is a subset of machine learning using multi-layered neural networks.\n",
      "\n",
      "2. (Similarity: 0.3899)\n",
      "   Machine learning models require large amounts of training data to perform well.\n",
      "\n",
      "3. (Similarity: 0.3823)\n",
      "   Natural language processing enables computers to understand human language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings/np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create index with inner product (equivalent to cosine for normalized vectors)\n",
    "index_cosine = faiss.IndexFlatIP(dimension)\n",
    "index_cosine.add(embeddings_normalized)\n",
    "\n",
    "#Search with normalized query\n",
    "query_embedding_normalized = query_embedding/np.linalg.norm(query_embedding)\n",
    "scores, indices = index_cosine.search(query_embedding_normalized, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {k} results with cosine similarity:\\n\")\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):\n",
    "    print(f\"{i}. (Similarity: {score:.4f})\")\n",
    "    print(f\"   {documents[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568ae07",
   "metadata": {},
   "source": [
    "### Saving and Loading FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df366ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved to disk\n",
      "Documents saved\n"
     ]
    }
   ],
   "source": [
    "#Save index to disk\n",
    "faiss.write_index(index_cosine, \"my_faiss_index.bin\")\n",
    "print(\"Index saved to disk\")\n",
    "\n",
    "# Save documents separately (FAISS only stores vectors not text)\n",
    "import pickle\n",
    "with open(\"documents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(documents, f)\n",
    "print(\"Documents saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d306318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: 10 vectors\n",
      "Documents loaded: 10 documents\n"
     ]
    }
   ],
   "source": [
    "# Load index from disk\n",
    "loaded_index = faiss.read_index(\"my_faiss_index.bin\")\n",
    "print(f\"Index loaded: {loaded_index.ntotal} vectors\")\n",
    "\n",
    "# Load documents \n",
    "with open(\"documents.pkl\", \"rb\") as f:\n",
    "    loaded_documents = pickle.load(f)\n",
    "print(f\"Documents loaded: {len(loaded_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f5df2",
   "metadata": {},
   "source": [
    "## Chroma Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867e0328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromadb version: 1.3.7\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "print(f\"Chromadb version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c80f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_documents\n",
      "Current count: 0 documents\n"
     ]
    }
   ],
   "source": [
    "#Create Chroma client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"my_documents\",\n",
    "    metadata={\"description\": \"Sample document collection\"}\n",
    "\n",
    ")\n",
    "print(collection.name)\n",
    "print(f\"Current count: {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973a6cf",
   "metadata": {},
   "source": [
    "### Add Documents to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473978ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [23:16<00:00, 59.6kiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data science.\",\n",
    "    \"Machine learning models require large amounts of training data to perform well.\",\n",
    "    \"Neural networks are inspired by the structure of the human brain.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning is a subset of machine learning using multi-layered neural networks.\"\n",
    "]\n",
    "\n",
    "# metadata for each document\n",
    "metadatas = [\n",
    "    {\"category\": \"programming\", \"topic\": \"python\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"machine learning\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"neural networks\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"NLP\"},\n",
    "    {\"category\": \"AI\", \"topic\": \"deep learning\"}\n",
    "]\n",
    "\n",
    "#IDs for each document\n",
    "ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Add to collection (Chroma handles embedding automatically!)\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids = ids\n",
    ")\n",
    "\n",
    "print(f\"Total documents: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed191e7",
   "metadata": {},
   "source": [
    "### Query Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b1c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is artificial intelligence?\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. (Distance: 1.1505)\n",
      "   Document: Deep learning is a subset of machine learning using multi-layered neural networks.\n",
      "   Metadata: {'topic': 'deep learning', 'category': 'AI'}\n",
      "\n",
      "2. (Distance: 1.2408)\n",
      "   Document: Natural language processing enables computers to understand human language.\n",
      "   Metadata: {'topic': 'NLP', 'category': 'AI'}\n",
      "\n",
      "3. (Distance: 1.2560)\n",
      "   Document: Neural networks are inspired by the structure of the human brain.\n",
      "   Metadata: {'category': 'AI', 'topic': 'neural networks'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts = ['What is artificial intelligence?'],\n",
    "    n_results =3\n",
    ")\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0], # we used [0] to extract the response for the first query because chromadb gives its response as a list of list, in order to be able to isolate the response for each query.\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    ",\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3539ecfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Deep learning is a subset of machine learning using multi-layered neural networks.',\n",
       "  'Natural language processing enables computers to understand human language.',\n",
       "  'Neural networks are inspired by the structure of the human brain.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9836d",
   "metadata": {},
   "source": [
    "### Filtering with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c990132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about AI (filtered by category='AI')\n",
      "\n",
      "Results:\n",
      "\n",
      "1. Deep learning is a subset of machine learning using multi-layered neural networks.\n",
      "    Categpry: AI, Topic: deep learning\n",
      "\n",
      "2. Natural language processing enables computers to understand human language.\n",
      "    Categpry: AI, Topic: NLP\n",
      "\n",
      "3. Neural networks are inspired by the structure of the human brain.\n",
      "    Categpry: AI, Topic: neural networks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Tell me about AI\"],\n",
    "    n_results=3,\n",
    "    where={\"category\": \"AI\"} # only returns documents in the AI category\n",
    ")\n",
    "\n",
    "print(\"Query: Tell me about AI (filtered by category='AI')\\n\")\n",
    "print(\"Results:\\n\")\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0]\n",
    "),1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "    print(f\"    Categpry: {metadata[\"category\"]}, Topic: {metadata[\"topic\"]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85116a0e",
   "metadata": {},
   "source": [
    "### Using Custom Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12dacfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Collection.add() got an unexpected keyword argument 'id'. Did you mean 'ids'?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m collection_custom = client.get_or_create_collection(\n\u001b[32m     10\u001b[39m     name = \u001b[33m\"\u001b[39m\u001b[33mcustom_embeddings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     embedding_function=sentence_trans_ef\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Add documents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mcollection_custom\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCollection with custom embeddings created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDocumnts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_custom.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Collection.add() got an unexpected keyword argument 'id'. Did you mean 'ids'?"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "#Use sentence-transformers embedding functions\n",
    "sentence_trans_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name= \"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create new collection with custom embedding function\n",
    "collection_custom = client.get_or_create_collection(\n",
    "    name = \"custom_embeddings\",\n",
    "    embedding_function=sentence_trans_ef\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "collection_custom.add(\n",
    "    documents = documents,\n",
    "    metadatas=metadatas,\n",
    "    id=ids\n",
    ")\n",
    "\n",
    "print(f\"Collection with custom embeddings created\")\n",
    "print(f\"Documnts: {collection_custom.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86af554",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection_custom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Query the collection\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mcollection_custom\u001b[49m.query(\n\u001b[32m      3\u001b[39m     query_texts=[\u001b[33m\"\u001b[39m\u001b[33mWhat is aritifical intelligenc?\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m     n_results=\u001b[32m3\u001b[39m,\n\u001b[32m      5\u001b[39m     include=[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'collection_custom' is not defined"
     ]
    }
   ],
   "source": [
    "#Query the collection\n",
    "results = collection_custom.query(\n",
    "    query_texts=[\"What is aritifical intelligenc?\"],\n",
    "    n_results=3,\n",
    "    include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection_custom.query(\n",
    "    query_texts=[\"What is artificial intelligence?\"],\n",
    "    n_results=3,\n",
    "    include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"Query: What is artificial intelligence?\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. (Distance: {distance:.4f})\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a6da2",
   "metadata": {},
   "source": [
    "### Update and delete documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45683f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a document\n",
    "collection.update(\n",
    "    ids=[\"doc_0\"],\n",
    "    documents=[\"Python is an amazing programming language for AI and data science!\"],\n",
    "    metadatas=[{\"category\": \"programming\", \"topic\": \"python\", \"updated\": True}]\n",
    ")\n",
    "print(\"Document updated\")\n",
    "\n",
    "# #Delete a document\n",
    "# collection.delete(ids=[\"doc_4\"])\n",
    "# print(\"Document deleted\")\n",
    "\n",
    "print(f\"Total documents after updatae {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3610e9",
   "metadata": {},
   "source": [
    "\n",
    "## Building  a Complete RAG Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2a0b4",
   "metadata": {},
   "source": [
    "### RAG Retriever with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c23f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, collection_name=\"rag_collection\", persist_dir=\"./rag_db\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG retriever with Chroma.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Create chroma client\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        #Create collection with sentence-transformers\n",
    "        embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embedding_fn\n",
    "        )\n",
    "\n",
    "        print(f\"RAG Retriever initialized\")\n",
    "        print(f\"Collection: {collection_name}\")\n",
    "        print(f\"Current documents: {self.collection.count()}\")\n",
    "        print(f\"Data persisted to: {persist_dir}/\")\n",
    "\n",
    "    def chunk_text(self, text, chunk_size=500):\n",
    "        \"\"\"\n",
    "        Simple sentence-based chunking from Module 2.\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        return chunks\n",
    "    \n",
    "    def add_document(self, text, metadata=None, source_name=\"unknown\"):\n",
    "        \"\"\"\n",
    "        Add a document (chunks it automatically).\n",
    "        \"\"\"\n",
    "        #Chunk the document\n",
    "        chunks = self.chunk_text(text)\n",
    "        # Prepare data for Chroma\n",
    "        ids = [f\"{source_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        metadatas =[\n",
    "            {\n",
    "                \"source\": source_name,\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                **(metadata or {}) # if metadat is None, it creates an empty dictionary instead of crashing\n",
    "            }\n",
    "            for i in range(len(chunks))\n",
    "        ]\n",
    "\n",
    "        #Add to collection\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\"Added document '{source_name}': {len(chunks)} chunks\")\n",
    "        return len(chunks)\n",
    "    \n",
    "    def retrieve(self, query, top_k=3, filter_metadata=None):\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks for a query.\n",
    "        \"\"\"\n",
    "        results= self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k,\n",
    "            where=filter_metadata\n",
    "        )\n",
    "\n",
    "        return{\n",
    "            'documents': results['documents'][0],\n",
    "            'metadatas': results['metadatas'][0],\n",
    "            'distance': results['distances'][0]\n",
    "        }\n",
    "    \n",
    "    def format_context(self, retrieved_results):\n",
    "        \"\"\"\n",
    "        Format retrieved chunks for LLM prompt.\n",
    "        \"\"\"\n",
    "        context = \"Context from retrieved documents: \\n\\n\"\n",
    "\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            retrieved_results['documents'],\n",
    "            retrieved_results['metadatas'],\n",
    "            retrieved_results['distances']\n",
    "        ),1):\n",
    "            source = metadata.get('source', 'unknown')\n",
    "            context += f\"[{i} ]From {source} (Relevance: {1/(1+distance):.3f}):\\n\"\n",
    "            context += f\"{doc}\\n\\n\"\n",
    "        return context\n",
    "    \n",
    "    print(\"RAG Retriever class defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
