{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b699bda6",
   "metadata": {},
   "source": [
    "# Agentic RAG with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75b7cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33983b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"paid_api\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API_Key not found. Please set it in your .env file\")\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f303e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "## Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    api_key = api_key\n",
    ")\n",
    "print(f\"LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0deae",
   "metadata": {},
   "source": [
    "## Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3bc53e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages from PDF.\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"c:\\Users\\owner\\Downloads\\Article_Research\\Artificial Intelligence in the Energy Industry.pdf\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found {file_path}\")\n",
    "    print(\"Please update the file_path variable with your PDF file.\")\n",
    "\n",
    "    #Creating sample documents for demo\n",
    "    from langchain_core.documents import Document\n",
    "    pages = [\n",
    "        Document(page_content=\"Biochemistry is the study of chemical processes in living organisms.\",\n",
    "                 metadata={\"page\":1}),\n",
    "        Document(page_content= \"Proteins are made of amino acids and perform many functions in cells.\",\n",
    "                 metadata = {\"page\":2}),\n",
    "        Document(page_content=\"DNA stores genetic information using four nucleotide bases.\",\n",
    "                 metadata={\"page\":3})\n",
    "    ]\n",
    "    print(\"Using sample documents for demo\")\n",
    "else:\n",
    "    #Load the pdf\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages= []\n",
    "\n",
    "    #Load pages (async loading)\n",
    "    async for page in loader.alazy_load(): # async prevents memory overload by processing pages from a PDF one by one instead of all at once\n",
    "        pages.append(page)\n",
    "    print(f\"Loaded {len(pages)} pages from PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e53744",
   "metadata": {},
   "source": [
    "### Split into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2fa065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 22 chunks\n",
      "\n",
      " Sample chunk:\n",
      "What is Artiﬁcial Intelligence in\n",
      "the Energy Industry ?\n",
      "Deﬁnition\n",
      "In recent years, Artiﬁcial Intelligence (AI) has gained relevance in a wide variety of\n",
      "sectors. However, deﬁning the term poses some d...\n"
     ]
    }
   ],
   "source": [
    "# Create text_splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "#Split documents\n",
    "doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Created {len(doc_splits)} chunks\")\n",
    "print(f\"\\n Sample chunk:\")\n",
    "print(f\"{doc_splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44040fcb",
   "metadata": {},
   "source": [
    "## Create Vector Store (Chromadb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac3ff38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model initialized\n"
     ]
    }
   ],
   "source": [
    "#Initilize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    api_key=api_key\n",
    ")\n",
    "print(\"Embeddings model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1195c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 22 chunks\n",
      "Persisted to: ./chroma_db_agentic_rag\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma vector stor\n",
    "chroma_path = \"./chroma_db_agentic_rag\"\n",
    "\n",
    "#Create vector store from documents\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"agentic_rag_docs\",\n",
    "    persist_directory=chroma_path,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "#Add documents\n",
    "vectorstore.add_documents(documents=doc_splits)\n",
    "print(f\"Vector store created with {len(doc_splits)} chunks\")\n",
    "print(f\"Persisted to: {chroma_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd0171",
   "metadata": {},
   "source": [
    "### Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c340af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is artificial intelligence?\n",
      "\n",
      "Top result:\n",
      "Even with this deﬁnition, the term remains diﬃcult to grasp. In practice, therefore,\n",
      "it is common to speak of strong or weak AI. A strong AI is one in which the\n",
      "application has all aspects associated ...\n",
      "\n",
      "Retrieval working!\n"
     ]
    }
   ],
   "source": [
    "# Test the vector store\n",
    "test_query = \"What is artificial intelligence?\"\n",
    "test_results = vectorstore.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nTop result:\")\n",
    "print(f\"{test_results[0].page_content[:200]}...\")\n",
    "print(f\"\\nRetrieval working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ade516",
   "metadata": {},
   "source": [
    "## Create Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeccd3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval tool created\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for relevant documents in the knowledge base.\n",
    "    \n",
    "    Use this tool when you need information from the document collection\n",
    "    to answer the user's question. Do NOT use this for:\n",
    "    - General knowledge questions\n",
    "    - Greetings or small talk\n",
    "    - Simple calculations\n",
    "    \n",
    "    Args:\n",
    "        query: The search query describing what information is needed\n",
    "        \n",
    "    Returns:\n",
    "        Relevant document excerpts that can help answer the question\n",
    "    \"\"\"\n",
    "    # Use MMR (Maximum Marginal Relevance) for diverse results\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type = \"mmr\",\n",
    "        search_kwargs ={\"k\":5, \"fetch_k\":10}\n",
    "    )\n",
    "\n",
    "    # Retrieve documents\n",
    "    results = retriever.invoke(query)\n",
    "\n",
    "    if not results:\n",
    "        return \"No relevant documents found\"\n",
    "    \n",
    "    formatted = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"Document {i+1}:\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(results)\n",
    "    )\n",
    "    return formatted\n",
    "print(\"Retrieval tool created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cdefa",
   "metadata": {},
   "source": [
    "### Test the Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c19bab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result (first 300 chars): \n",
      "Document 1:\n",
      "Even with this deﬁnition, the term remains diﬃcult to grasp. In practice, therefore,\n",
      "it is common to speak of strong or weak AI. A strong AI is one in which the\n",
      "application has all aspects associated with human intelligence, such as the ability\n",
      "to draw logical conclusions, the existence ...\n"
     ]
    }
   ],
   "source": [
    "# Test tool directly\n",
    "test_result = retrieve_documents.invoke({\"query\": \"What is artificial intelligence?\"})\n",
    "print(f\"Tool result (first 300 chars): \\n{test_result[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b021689",
   "metadata": {},
   "source": [
    "## Build the Agentic RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d31fb",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c2a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt configured\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SystemMessage(content=\"\"\"You are a helpful assistant with access to a document retrieval tool.\n",
    "\n",
    "RETRIEVAL DECISION RULES:\n",
    "\n",
    "DO NOT retrieve for:\n",
    "- Greetings: \"Hello\", \"Hi\", \"How are you\"\n",
    "- Questions about your capabilities: \"What can you help with?\", \"What do you do?\"\n",
    "- Simple math or general knowledge: \"What is 2+2?\"\n",
    "- Casual conversation: \"Thank you\", \"Goodbye\"\n",
    "\n",
    "DO retrieve for:\n",
    "- Questions asking for specific information that would be in documents\n",
    "- Requests for facts, definitions, or explanations about specialized topics\n",
    "- Any question where citing sources would improve the answer\n",
    "\n",
    "Rule of thumb: If the user is asking for information (not just chatting), retrieve first.\n",
    "\n",
    "When you retrieve documents, cite them in your answer. If documents don't contain the answer, say so.\n",
    "\"\"\")\n",
    "\n",
    "print(\"System prompt configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7e41d",
   "metadata": {},
   "source": [
    "### Agent Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind tool to LLM\n",
    "tools = [retrieve_documents]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    Assistant node - decides whether to retrieve or answer directly.\n",
    "    \"\"\"\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
