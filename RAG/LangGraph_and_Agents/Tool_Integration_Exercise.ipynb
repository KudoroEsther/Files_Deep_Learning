{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0154a7",
   "metadata": {},
   "source": [
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8917a1",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018c8f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "import os\n",
    "import re\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92681bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('paid_api')\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found!\")\n",
    "print(\"API Key loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbe2ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized gpt-40-mini\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-40-mini\",\n",
    "    temperature = 0.7,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "print(f\"LLM initialized {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8ee6b",
   "metadata": {},
   "source": [
    "## Creating Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70cc0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ tool\n",
    "def weather_tool(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns simulated weather for a given city.\n",
    "    Use this tool when the weather of a given city is required.\n",
    "\n",
    "    Args:\n",
    "        city: The given city\n",
    "    \n",
    "    Returns:\n",
    "        A simulated weather for the city\n",
    "    \"\"\"\n",
    "\n",
    "    def simulate_weather(city: str) ->dict:\n",
    "        \"\"\"\n",
    "        Generate deterministic fake weather data for a city.\n",
    "        \"\"\"\n",
    "        random.seed(city.lower())\n",
    "\n",
    "        conditions = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Thunderstorm\", \"Hazy\"]\n",
    "        \n",
    "        return {\n",
    "            \"city\": city.title(),\n",
    "            \"temperature_c\": random.randint(22, 36),\n",
    "            \"condition\": random.choice(conditions),\n",
    "            \"humidity_percent\": random.randint(40, 90)\n",
    "        }\n",
    "    \n",
    "    weather = simulate_weather(city)\n",
    "    return (\n",
    "        f\"Weather in {weather['city']}:\\n\"\n",
    "        f\"- Temperature: {weather['temperature_c']}°C\\n\"\n",
    "        f\"- Condition: {weather['condition']}\\n\"\n",
    "        f\"- Humidity: {weather['humidity_percent']}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b224a4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Function must have a docstring if description not provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129;43m@tool\u001b[39;49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mdictionary\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[33;43;03m    Accepts a question and returns a definition from a simulated dictionary.\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;43;03m    - \"What does embedding mean?\"\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\owner\\Desktop\\Files_Deep_Learning\\a_deep\\Lib\\site-packages\\langchain_core\\tools\\convert.py:355\u001b[39m, in \u001b[36mtool\u001b[39m\u001b[34m(name_or_callable, runnable, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, extras, *args)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name_or_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(name_or_callable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name_or_callable, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# Used as a decorator without parameters\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;66;03m# @tool\u001b[39;00m\n\u001b[32m    353\u001b[39m         \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[32m    354\u001b[39m         \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_tool_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_callable\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_callable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_callable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    357\u001b[39m         \u001b[38;5;66;03m# Used with a new name for the tool\u001b[39;00m\n\u001b[32m    358\u001b[39m         \u001b[38;5;66;03m# @tool(\"search\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m         \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[32m    366\u001b[39m         \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _create_tool_factory(name_or_callable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\owner\\Desktop\\Files_Deep_Learning\\a_deep\\Lib\\site-packages\\langchain_core\\tools\\convert.py:298\u001b[39m, in \u001b[36mtool.<locals>._create_tool_factory.<locals>._tool_factory\u001b[39m\u001b[34m(dec_func)\u001b[39m\n\u001b[32m    295\u001b[39m     schema = args_schema\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infer_schema \u001b[38;5;129;01mor\u001b[39;00m args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStructuredTool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextras\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# If someone doesn't want a schema applied, we must treat it as\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# a simple string->string function\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dec_func.\u001b[34m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\owner\\Desktop\\Files_Deep_Learning\\a_deep\\Lib\\site-packages\\langchain_core\\tools\\structured.py:228\u001b[39m, in \u001b[36mStructuredTool.from_function\u001b[39m\u001b[34m(cls, func, coroutine, name, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    227\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mFunction must have a docstring if description not provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Only apply if using the function's docstring\u001b[39;00m\n\u001b[32m    231\u001b[39m     description_ = textwrap.dedent(description_).strip()\n",
      "\u001b[31mValueError\u001b[39m: Function must have a docstring if description not provided."
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def dictionary(question: str) -> str:\n",
    "    pass\n",
    "    \"\"\"\n",
    "    Accepts a question and returns a definition from a simulated dictionary.\n",
    "\n",
    "    Example inputs:\n",
    "    - \"What is the meaning of agent?\"\n",
    "    - \"Define RAG\"\n",
    "    - \"What does embedding mean?\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Simulated dictionary\n",
    "    dictionary = {\n",
    "        \"agent\": \"An entity that perceives its environment and acts upon it.\",\n",
    "        \"tool\": \"A function or capability an agent can use to perform a task.\",\n",
    "        \"rag\": \"Retrieval-Augmented Generation, combining retrieval with generation.\",\n",
    "        \"llm\": \"Large Language Model trained on vast amounts of text data.\",\n",
    "        \"embedding\": \"A numerical representation of text capturing semantic meaning.\"\n",
    "    }\n",
    "\n",
    "    # Normalize question\n",
    "    word = question.lower().strip()\n",
    "\n",
    "\n",
    "    # Lookup\n",
    "    if word in dictionary:\n",
    "        return f\"{word.title()}: {dictionary[word]}\"\n",
    "    else:\n",
    "        return f\"{word.title()}: Definition not found in the simulated dictionary.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8401779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the web using DuckDuckGo and returns summarized results.\n",
    "\n",
    "    Example inputs:\n",
    "    - \"Latest news on AI\"\n",
    "    - \"What is LangGraph?\"\n",
    "    - \"Weather patterns in Nigeria\"\n",
    "    \"\"\"\n",
    "\n",
    "    results_text = []\n",
    "\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(\n",
    "            query,\n",
    "            region=\"wt-wt\",\n",
    "            safesearch=\"moderate\",\n",
    "            max_results=5\n",
    "        )\n",
    "\n",
    "        for r in results:\n",
    "            title = r.get(\"title\", \"No title\")\n",
    "            snippet = r.get(\"body\", \"No summary\")\n",
    "            source = r.get(\"href\", \"No link\")\n",
    "\n",
    "            results_text.append(\n",
    "                f\"Title: {title}\\n\"\n",
    "                f\"Summary: {snippet}\\n\"\n",
    "                f\"Source: {source}\\n\"\n",
    "            )\n",
    "\n",
    "    if not results_text:\n",
    "        return \"No results found.\"\n",
    "\n",
    "    return \"\\n---\\n\".join(results_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b3a06",
   "metadata": {},
   "source": [
    "## Binding tools to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools= [weather_tool, dictionary, web_search]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(f\"LLM bound to {len(tools)} tools\")\n",
    "print(f\"Tools: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4654a1",
   "metadata": {},
   "source": [
    "## Defining the Assistant Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = SystemMessage(content=\"\"\"You are a helpful assitant with access to tools.\n",
    "When asked to perform for the weather, use the weather tool.\n",
    "When asked to define something, use the dictionary tool.\n",
    "When asked to search the internet, use the web search tool.\n",
    "\n",
    "Only use tools when necessary - for simple questions, answer directly.\"\"\")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    Assistant node - decides whether to use tools or answer directly.\n",
    "    \"\"\"\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"Assistant mode defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be3418",
   "metadata": {},
   "source": [
    "## Implementing Conditional Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState)-> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Decide next step based on last message.\n",
    "    \n",
    "    If LLM called a tool → go to 'tools' node\n",
    "    If LLM provided final answer → go to END\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    # check if LLM made tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "print(\"Conditional routing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c227f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", \"__end__\": END}\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "agent= builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Agent graph compiled with tools and memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc758e3",
   "metadata": {},
   "source": [
    "## The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_agent(user_input: str, thread_id: str = \"default\"):\n",
    "    \"\"\"\n",
    "    Run the agent and display the conversation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            continue\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if message.tool_calls:\n",
    "                print(f\"Agent: [Calling tool: {message.tool_calls[0][\"name\"]}]\")\n",
    "            else:\n",
    "                print(f\"Agent: {message.content}\")\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"Tool Result: {message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
